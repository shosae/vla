# RoboVLMs ìƒì„¸ ë¶„ì„ ë¦¬í¬íŠ¸

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡° ê°œìš”

RoboVLMsëŠ” Vision-Language-Action (VLA) ëª¨ë¸ì„ ìœ„í•œ ì¢…í•©ì ì¸ í”„ë ˆì„ì›Œí¬ë¡œ, ë¡œë´‡ ì œì–´ë¥¼ ìœ„í•œ ë©€í‹°ëª¨ë‹¬ AI ì‹œìŠ¤í…œì„ êµ¬í˜„í•©ë‹ˆë‹¤.

```
RoboVLMs/
â”œâ”€â”€ configs/                    # ì„¤ì • ë° êµ¬ì„± íŒŒì¸íŠœë‹ ì„¤ì •
â”‚   â”œâ”€â”€ calvin_finetune/       # CALVIN ë°ì´í„°ì…‹ íŒŒì¸íŠœë‹ ì„¤ì •
â”‚   â”œâ”€â”€ data/                  # ë°ì´í„° ê´€ë ¨ ì„¤ì •
â”‚   â””â”€â”€ oxe_training/          # Open-X Embodiment í›ˆë ¨ ì„¤ì •
â”œâ”€â”€ robovlms/                  # ë©”ì¸ íŒ¨í‚¤ì§€
â”‚   â”œâ”€â”€ data/                  # ë°ì´í„° ì²˜ë¦¬ ëª¨ë“ˆ
â”‚   â”œâ”€â”€ model/                 # ëª¨ë¸ ì•„í‚¤í…ì²˜
â”‚   â”œâ”€â”€ train/                 # í›ˆë ¨ ì‹œìŠ¤í…œ
â”‚   â””â”€â”€ utils/                 # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
â”œâ”€â”€ vla_test/                  # VLA í…ŒìŠ¤íŠ¸ ë° ì•¡ì…˜ íŒŒì‹±
â””â”€â”€ scripts/                   # ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
```

## ğŸ¯ VLAì—ì„œ ì •ì±…(Policy)ì˜ ê°œë…ê³¼ êµ¬í˜„

### 1. ì •ì±…(Policy)ì´ë€?

**ì •ì±…(Policy)**ì€ VLA ì‹œìŠ¤í…œì˜ í•µì‹¬ êµ¬ì„±ìš”ì†Œë¡œ, **"ì£¼ì–´ì§„ ìƒí™©(ë¹„ì „+ì–¸ì–´)ì—ì„œ ë¡œë´‡ì´ ì–´ë–¤ í–‰ë™ì„ ì·¨í• ì§€ ê²°ì •í•˜ëŠ” ì˜ì‚¬ê²°ì • í•¨ìˆ˜"**ì…ë‹ˆë‹¤.

#### ìˆ˜í•™ì  ì •ì˜
```python
Ï€(action | vision, language, history) 
= ì´ë¯¸ì§€, ì–¸ì–´ ëª…ë ¹, ì´ì „ í–‰ë™ íˆìŠ¤í† ë¦¬ê°€ ì£¼ì–´ì¡Œì„ ë•Œ 
  ë‹¤ìŒì— ì·¨í•  ìµœì ì˜ ì•¡ì…˜ì„ ì„ íƒí•˜ëŠ” í™•ë¥  ë¶„í¬
```

#### ì •ì±…ì˜ ì—­í• 
1. **ì¸ì‹**: í˜„ì¬ ìƒí™© íŒŒì•… (ë¹„ì „ + ì–¸ì–´ ì´í•´)
2. **ì¶”ë¡ **: ëª©í‘œ ë‹¬ì„±ì„ ìœ„í•œ ìµœì  í–‰ë™ ê³„íš
3. **ì‹¤í–‰**: êµ¬ì²´ì ì¸ ë¡œë´‡ ì œì–´ ëª…ë ¹ ìƒì„±

### 2. RoboVLMsì˜ ì •ì±… ì•„í‚¤í…ì²˜

#### ì •ì±… ì‹œìŠ¤í…œ êµ¬ì¡°
```python
class BaseRoboVLM(nn.Module):
    def _init_heads(self):
        # ì •ì±… í—¤ë“œ ì´ˆê¸°í™”
        action_head = self._build_policy_head()
        
    def forward(self, vision_x, lang_x, ...):
        # 1. ë©€í‹°ëª¨ë‹¬ íŠ¹ì§• ìœµí•©
        fused_features = self.encode_multimodal(vision_x, lang_x)
        
        # 2. ì •ì±… í—¤ë“œë¥¼ í†µí•œ ì•¡ì…˜ ì˜ˆì¸¡
        predicted_actions = self.action_head(fused_features)
        
        return predicted_actions
```

#### ì§€ì›ë˜ëŠ” ì •ì±… í—¤ë“œ íƒ€ì…ë“¤

**1. FCDecoder (Fully Connected Policy)**
```python
class FCDecoder(BasePolicyHead):
    def __init__(self, in_features, action_dim, ...):
        # ì™„ì „ì—°ê²°ì¸µ ê¸°ë°˜ ì •ì±…
        self.actions = MLPTanhHead(hidden_size, action_dim-1)  # íŒ” ì œì–´
        self.gripper = MLPSigmoidHead(hidden_size, 1)         # ê·¸ë¦¬í¼ ì œì–´
        
    def forward(self, features):
        arm_actions = self.actions(features)      # [-1, 1] ë²”ìœ„ ì—°ì†ê°’
        gripper_action = self.gripper(features)   # [0, 1] í™•ë¥ ê°’
        return arm_actions, gripper_action
```

**2. LSTMDecoder (Sequential Policy)**
```python
class LSTMDecoder(BasePolicyHead):
    def __init__(self, window_size, fwd_pred_next_n, ...):
        # ì‹œí€€ìŠ¤ ê¸°ë°˜ ì •ì±… (ì‹œê°„ì  ì˜ì¡´ì„± ê³ ë ¤)
        self.rnn = LSTM(input_size, hidden_size, num_layers)
        self.actions = MLPTanhHead(hidden_size, action_dim)
        
    def forward(self, feature_sequence):
        # ê³¼ê±° NìŠ¤í…ì˜ íŠ¹ì§•ì„ ê³ ë ¤í•˜ì—¬ ë¯¸ë˜ ì•¡ì…˜ ì˜ˆì¸¡
        lstm_output, hidden = self.rnn(feature_sequence)
        predicted_actions = self.actions(lstm_output[-1])
        return predicted_actions
```

**3. GPTDecoder (Transformer Policy)**
```python
class GPTDecoder(BasePolicyHead):
    def __init__(self, window_size, ...):
        # GPT ìŠ¤íƒ€ì¼ íŠ¸ëœìŠ¤í¬ë¨¸ ì •ì±…
        self.gpt = GPT2Model(config)
        self.actions = MLPTanhHead(hidden_size, action_dim)
        
    def forward(self, feature_sequence):
        # ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ ì¤‘ìš”í•œ ê³¼ê±° ì •ë³´ì— ì§‘ì¤‘
        transformer_output = self.gpt(feature_sequence)
        actions = self.actions(transformer_output)
        return actions
```

**4. DiscreteDecoder (Discrete Action Policy)**
```python
class DiscreteDecoder(BasePolicyHead):
    def __init__(self, tokenizer, n_bin=256, ...):
        # ì´ì‚°ì  ì•¡ì…˜ ê³µê°„ ì •ì±…
        self.action_tokenizer = ActionTokenizer(tokenizer, bins=n_bin)
        
    def forward(self, features):
        # ì—°ì† ì•¡ì…˜ì„ ì´ì‚°ì  í† í°ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì–¸ì–´ëª¨ë¸ì²˜ëŸ¼ ì²˜ë¦¬
        action_logits = self.classifier(features)  # [bs, seq_len, vocab_size]
        action_tokens = torch.argmax(action_logits, dim=-1)
        decoded_actions = self.action_tokenizer.decode(action_tokens)
        return decoded_actions
```

### 3. ì •ì±…ì˜ í•™ìŠµ ê³¼ì •

#### ì†ì‹¤ í•¨ìˆ˜ êµ¬ì¡°
```python
def policy_loss(predicted_actions, ground_truth_actions, attention_mask):
    # 1. íŒ” ì œì–´ ì†ì‹¤ (Huber Loss - ì—°ì†ê°’)
    arm_loss = F.huber_loss(predicted_actions[..., :6], gt_actions[..., :6])
    
    # 2. ê·¸ë¦¬í¼ ì œì–´ ì†ì‹¤ (Binary Cross Entropy - ì´ì§„ê°’)
    gripper_loss = F.binary_cross_entropy_with_logits(
        predicted_actions[..., -1], gt_actions[..., -1]
    )
    
    # 3. ë§ˆìŠ¤í‚¹ëœ ìœ„ì¹˜ë§Œ ê³„ì‚°
    if attention_mask is not None:
        arm_loss = arm_loss[attention_mask].mean()
        gripper_loss = gripper_loss[attention_mask].mean()
    
    return {
        'loss_arm': arm_loss,
        'loss_gripper': gripper_loss,
        'total_loss': arm_loss + gripper_loss
    }
```

### 4. ì •ì±…ì˜ ì‹¤í–‰ íë¦„

#### í›ˆë ¨ ì‹œ ì •ì±… ì‘ë™
```python
def training_step(self, batch):
    # 1. ë©€í‹°ëª¨ë‹¬ ì…ë ¥ ì²˜ë¦¬
    vision_features = self.encode_images(batch['images'])
    text_features = self.encode_text(batch['instructions'])
    
    # 2. íŠ¹ì§• ìœµí•©
    fused_features = self.backbone(vision_features, text_features)
    
    # 3. ì •ì±…ì„ í†µí•œ ì•¡ì…˜ ì˜ˆì¸¡
    predicted_actions = self.policy_head(fused_features)
    
    # 4. ì •ì±… ì†ì‹¤ ê³„ì‚°
    policy_loss = self.compute_policy_loss(predicted_actions, batch['actions'])
    
    return policy_loss
```

#### ì¶”ë¡  ì‹œ ì •ì±… ì‘ë™
```python
def inference(self, image, instruction):
    with torch.no_grad():
        # 1. ì…ë ¥ ì „ì²˜ë¦¬
        vision_x = self.preprocess_image(image)
        lang_x = self.tokenize_instruction(instruction)
        
        # 2. ì •ì±…ì„ í†µí•œ ì•¡ì…˜ ìƒì„±
        predicted_action = self.forward(vision_x, lang_x)
        
        # 3. ì•¡ì…˜ í›„ì²˜ë¦¬ ë° ì•ˆì „ì„± ê²€ì¦
        safe_action = self.validate_action(predicted_action)
        
        return safe_action
```

### 5. ì •ì±… ì„ íƒ ê¸°ì¤€

#### ì •ì±… í—¤ë“œ ì„ íƒ ê°€ì´ë“œë¼ì¸

| ì •ì±… íƒ€ì… | ì ìš© ìƒí™© | ì¥ì  | ë‹¨ì  |
|-----------|-----------|------|------|
| **FCDecoder** | ë‹¨ìˆœí•œ ì¦‰ì‹œ ë°˜ì‘ íƒœìŠ¤í¬ | ë¹ ë¥¸ ì¶”ë¡ , ê²½ëŸ‰í™” | ì‹œê°„ì  ë§¥ë½ ë¶€ì¡± |
| **LSTMDecoder** | ìˆœì°¨ì  í–‰ë™ì´ ì¤‘ìš”í•œ íƒœìŠ¤í¬ | ì‹œê°„ì  ì˜ì¡´ì„± ëª¨ë¸ë§ | ì¥ê¸° ì˜ì¡´ì„± í•œê³„ |
| **GPTDecoder** | ë³µí•©ì ì´ê³  ê¸´ ì‹œí€€ìŠ¤ íƒœìŠ¤í¬ | ê°•ë ¥í•œ ì‹œí€€ìŠ¤ ëª¨ë¸ë§ | ê³„ì‚° ë¹„ìš© ë†’ìŒ |
| **DiscreteDecoder** | ì–¸ì–´ëª¨ë¸ê³¼ í†µí•©ëœ ì‹œìŠ¤í…œ | ì–¸ì–´-ì•¡ì…˜ í†µí•© í•™ìŠµ | ì—°ì†ì„± ì •ë³´ ì†ì‹¤ |

### 6. ì •ì±… ìµœì í™” ê¸°ë²•

#### ì •ì±… ì •ê·œí™”
```python
# ì•¡ì…˜ ê°’ ì •ê·œí™”
def normalize_actions(actions):
    # [-1, 1] ë²”ìœ„ë¡œ ì •ê·œí™”
    normalized = (actions - action_min) / (action_max - action_min) * 2 - 1
    return normalized

# Î¼-law ì••ì¶• (ìŒì„± ì²˜ë¦¬ì—ì„œ ì˜ê°)
def mu_law_encoding(actions, mu=255):
    return torch.sign(actions) * torch.log(1 + mu * torch.abs(actions)) / torch.log(1 + mu)
```

#### ì•ˆì „ì„± ë³´ì¥
```python
class ActionValidator:
    def validate_action(self, action):
        # 1. ì†ë„ ì œí•œ
        action.linear_x = torch.clamp(action.linear_x, -0.5, 0.5)
        action.angular_z = torch.clamp(action.angular_z, -1.0, 1.0)
        
        # 2. ì‹ ë¢°ë„ ê²€ì‚¬
        if action.confidence < 0.3:
            return self.get_safe_default_action()
            
        return action
```

## ğŸ§  VLA ëª¨ë¸ ì•„í‚¤í…ì²˜ ë¶„ì„

### 1. ëª¨ë¸ ë°±ë³¸ êµ¬ì¡°

RoboVLMsëŠ” ë‹¤ì–‘í•œ ë°±ë³¸ ëª¨ë¸ì„ ì§€ì›í•˜ëŠ” ëª¨ë“ˆëŸ¬ ì•„í‚¤í…ì²˜ë¥¼ ì±„íƒí•©ë‹ˆë‹¤:

#### ì§€ì›ë˜ëŠ” ë°±ë³¸ ëª¨ë¸ë“¤
- **RoboFlamingo**: OpenFlamingo ê¸°ë°˜ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸
- **RoboLLaVA**: LLaVA ê¸°ë°˜ ë¹„ì „-ì–¸ì–´ ëª¨ë¸  
- **RoboQwen**: Qwen ê¸°ë°˜ ëŒ€í™”í˜• AI ëª¨ë¸
- **RoboPaligemma**: PaLI ê³„ì—´ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸
- **RoboMoonDream**: ê²½ëŸ‰í™”ëœ ë¹„ì „-ì–¸ì–´ ëª¨ë¸
- **RoboUform**: Unified í˜•íƒœì˜ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸

#### ë°±ë³¸ ëª¨ë¸ì˜ ê³µí†µ ì¸í„°í˜ì´ìŠ¤

```python
class BaseRoboVLM(nn.Module):
    def __init__(self, configs, train_setup_configs, ...):
        # ê³µí†µ ì´ˆê¸°í™” êµ¬ì¡°
        self._init_backbone()      # ë°±ë³¸ ëª¨ë¸ ì´ˆê¸°í™”
        self._init_heads()         # ì •ì±… í—¤ë“œ ì´ˆê¸°í™”
        self._trainable_params_setup()  # í›ˆë ¨ íŒŒë¼ë¯¸í„° ì„¤ì •
    
    @property
    def hidden_size(self):         # ì€ë‹‰ì¸µ í¬ê¸°
    
    @property  
    def vision_tower(self):        # ë¹„ì „ ì¸ì½”ë”
    
    @property
    def text_tower(self):          # í…ìŠ¤íŠ¸ ì¸ì½”ë”
    
    def encode_images(self, images): # ì´ë¯¸ì§€ ì¸ì½”ë”©
    
    def forward(self, vision_x, lang_x, ...): # ìˆœì „íŒŒ
```

### 2. ë©€í‹°ëª¨ë‹¬ ì…ë ¥ ì²˜ë¦¬

#### ì´ë¯¸ì§€ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
```python
def encode_images(self, images, image_sizes=None):
    # ì…ë ¥: images: list of b,c,h,w or b,t,c,h,w
    # ì¶œë ¥: image_features: b, t, n, d
    
    # 1. ì´ë¯¸ì§€ ì „ì²˜ë¦¬
    # 2. ë¹„ì „ ì¸ì½”ë”ë¥¼ í†µí•œ íŠ¹ì§• ì¶”ì¶œ
    # 3. ì‹œí€€ìŠ¤ ì°¨ì›ìœ¼ë¡œ ì¬êµ¬ì„±
```

#### ì–¸ì–´ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
- í† í¬ë‚˜ì´ì €ë¥¼ í†µí•œ í…ìŠ¤íŠ¸ ì¸ì½”ë”©
- í”„ë¡¬í”„íŠ¸ ë¹Œë”ë¥¼ í†µí•œ ëŒ€í™” í˜•ì‹ êµ¬ì„±
- ì–´í…ì…˜ ë§ˆìŠ¤í¬ ìƒì„±

### 3. ì•¡ì…˜ ì˜ˆì¸¡ ì‹œìŠ¤í…œ

#### ì•¡ì…˜ í† í¬ë‚˜ì´ì € (Discrete Actions)
```python
class ActionTokenizer:
    def __init__(self, tokenizer, bins=256, min_action=-1, max_action=1):
        # ì—°ì†ì ì¸ ì•¡ì…˜ì„ ì´ì‚°ì ì¸ í† í°ìœ¼ë¡œ ë³€í™˜
        
    def encode_actions_to_token_ids(self, action):
        # ì•¡ì…˜ â†’ í† í° ID ë³€í™˜
        
    def decode_token_ids_to_actions(self, action_token_ids):
        # í† í° ID â†’ ì•¡ì…˜ ë³€í™˜
```

#### ì •ì±… í—¤ë“œ êµ¬ì¡°
- **FCDecoder**: ì™„ì „ì—°ê²°ì¸µ ê¸°ë°˜ ë””ì½”ë”
- **LSTMDecoder**: LSTM ê¸°ë°˜ ì‹œí€€ìŠ¤ ë””ì½”ë”  
- **GPTDecoder**: GPT ìŠ¤íƒ€ì¼ íŠ¸ëœìŠ¤í¬ë¨¸ ë””ì½”ë”
- **DiscreteDecoder**: ì´ì‚°ì  ì•¡ì…˜ ê³µê°„ ë””ì½”ë”

## ğŸ¯ ì•¡ì…˜ ê·œì • ë° ì²˜ë¦¬ ë°©ì‹

### 1. ì•¡ì…˜ íƒ€ì… ì •ì˜

VLA ì‹œìŠ¤í…œì—ì„œ ì§€ì›í•˜ëŠ” ì•¡ì…˜ íƒ€ì…ë“¤:

```python
class ActionType(Enum):
    MOVE = "move"           # ì´ë™
    TURN = "turn"           # íšŒì „
    STOP = "stop"           # ì •ì§€
    GRAB = "grab"           # ì¡ê¸°
    RELEASE = "release"     # ë†“ê¸°
    POINT = "point"         # ê°€ë¦¬í‚¤ê¸°
    LOOK = "look"           # ë³´ê¸°
    NAVIGATE = "navigate"   # ë„¤ë¹„ê²Œì´ì…˜
    AVOID = "avoid"         # íšŒí”¼
    UNKNOWN = "unknown"     # ë¯¸ì§€ì •
```

### 2. ì•¡ì…˜ ë°ì´í„° êµ¬ì¡°

```python
@dataclass
class RobotAction:
    action_type: ActionType    # ì•¡ì…˜ íƒ€ì…
    linear_x: float = 0.0      # ì„ í˜• ì†ë„ (ì „í›„)
    linear_y: float = 0.0      # ì„ í˜• ì†ë„ (ì¢Œìš°)
    angular_z: float = 0.0     # ê°ì†ë„ (íšŒì „)
    target_object: str = None  # ëª©í‘œ ê°ì²´
    confidence: float = 0.0    # ì‹ ë¢°ë„
    description: str = ""      # ì„¤ëª…
```

### 3. VLA ì¶œë ¥ íŒŒì‹± ì‹œìŠ¤í…œ

#### í…ìŠ¤íŠ¸ ê¸°ë°˜ ì•¡ì…˜ íŒŒì‹±
```python
class VLAActionParser:
    def parse_text_output(self, vla_output: str) -> RobotAction:
        # 1. ì•¡ì…˜ íƒ€ì… ê²°ì •
        action_type = self._determine_action_type(text)
        
        # 2. í‚¤ì›Œë“œ ë§¤ì¹­ì„ í†µí•œ ì•¡ì…˜ ë¶„ë¥˜
        # 3. ë°©í–¥ì„± ë° ì†ë„ ìˆ˜ì‹ì–´ ì²˜ë¦¬
        # 4. ì‹ ë¢°ë„ ê³„ì‚°
        
    def _determine_action_type(self, text: str) -> ActionType:
        # í‚¤ì›Œë“œ ê¸°ë°˜ ì•¡ì…˜ íƒ€ì… ê²°ì •
        for action_type, keywords in self.action_keywords.items():
            # í‚¤ì›Œë“œ ë§¤ì¹­ ìŠ¤ì½”ì–´ ê³„ì‚°
```

#### ì„¸ê·¸ë©˜í…Œì´ì…˜ í† í° ê¸°ë°˜ íŒŒì‹±
```python
def parse_segmentation_output(self, vla_output: str, image_width: int, image_height: int):
    # <loc0500><loc0300><loc0700><loc0600> í˜•íƒœì˜ ìœ„ì¹˜ í† í° íŒŒì‹±
    loc_tokens = re.findall(r"<loc(\d{4})>", vla_output)
    
    # ë°”ìš´ë”© ë°•ìŠ¤ì—ì„œ ì´ë™ ëª…ë ¹ ê³„ì‚°
    linear_x, linear_y, angular_z = self._calculate_movement_from_bbox(bbox, ...)
```

### 4. ì•¡ì…˜ ê³µê°„ ì²˜ë¦¬

#### ì—°ì† ì•¡ì…˜ ê³µê°„
- ì •ê·œí™”: `normalize_action(action, action_min=-1, action_max=1)`
- ì •ì¹™í™”: `regularize_action(x, x_mean, x_std)` 
- Î¼-law ì••ì¶•: `mu_law_companding(x, mu=255)`

#### ì´ì‚° ì•¡ì…˜ ê³µê°„
- ë¹ˆ ì–‘ìí™”: ì—°ì† ê°’ì„ 256ê°œ ë¹ˆìœ¼ë¡œ ë¶„í• 
- í† í° ë§¤í•‘: ê° ë¹ˆì„ ê³ ìœ  í† í° IDì— ë§¤í•‘
- ì–¸ì–´ ëª¨ë¸ í†µí•©: ì•¡ì…˜ í† í°ì„ í…ìŠ¤íŠ¸ í† í°ê³¼ í•¨ê»˜ ì²˜ë¦¬

## ğŸ”„ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

### 1. ì•¡ì…˜ ì˜ˆì¸¡ ë°ì´í„°ì…‹

```python
class ActionPredictionDataset(BaseTaskDataset):
    def __init__(self, 
                 window_size: int = 16,        # íˆìŠ¤í† ë¦¬ ìœˆë„ìš° í¬ê¸°
                 fwd_pred_next_n: int = 2,     # ì˜ˆì¸¡í•  ë¯¸ë˜ ìŠ¤í… ìˆ˜
                 organize_type: str = "segment", # "interleave" or "segment"
                 discrete: bool = True,         # ì´ì‚°/ì—°ì† ì•¡ì…˜
                 ...):
```

#### ë°ì´í„° ì¡°ì§í™” ë°©ì‹

**Segment ë°©ì‹**:
```
[íˆìŠ¤í† ë¦¬ ì´ë¯¸ì§€] + [ì–¸ì–´ ëª…ë ¹] + [íˆìŠ¤í† ë¦¬ ì•¡ì…˜] + [ë¯¸ë˜ ì•¡ì…˜ ì˜ˆì¸¡]
```

**Interleave ë°©ì‹**:
```
[ì´ë¯¸ì§€1] + [ì•¡ì…˜1] + [ì´ë¯¸ì§€2] + [ì•¡ì…˜2] + ... + [ì˜ˆì¸¡ ì•¡ì…˜]
```

### 2. ë°°ì¹˜ ë³€í™˜ ì‹œìŠ¤í…œ

```python
class ActionPredictionBatchTransform:
    def convert_image(self, images, image_mask):
        # ì´ë¯¸ì§€ í…ì„œ ë³€í™˜ ë° íˆìŠ¤í† ë¦¬ ì²˜ë¦¬
        
    def convert_action(self, action, action_mask):
        # ì•¡ì…˜ ì •ê·œí™”, ì •ì¹™í™”, Î¼-law ë³€í™˜
        
    def wrap_instruction_and_action_segment(self, task_description, action, action_mask):
        # ëª…ë ¹ì–´ì™€ ì•¡ì…˜ì„ í•˜ë‚˜ì˜ ì‹œí€€ìŠ¤ë¡œ ê²°í•©
```

## ğŸš€ ëª¨ë¸ ì‚¬ìš© ë°©ë²•

### 1. í›ˆë ¨ ì„¤ì •

```python
# ì„¤ì • ë¡œë“œ
configs = load_config("configs/calvin_finetune/roboflamingo_calvin.yaml")

# ëª¨ë¸ ì´ˆê¸°í™”
model = build_vlm(vlm_config, tokenizer_config, precision="bf16")

# ë°ì´í„° ëª¨ë“ˆ ì„¤ì •
data_module = GRDataModule(
    train_dataset=train_dataset_configs,
    val_dataset=val_dataset_configs,
    batch_size=batch_size,
    num_workers=num_workers
)

# í›ˆë ¨ ì‹¤í–‰
trainer = BaseTrainer(configs)
trainer.fit(model, data_module)
```

### 2. ì¶”ë¡  ê³¼ì •

```python
# ì´ë¯¸ì§€ì™€ ì–¸ì–´ ì…ë ¥ ì¤€ë¹„
vision_x = preprocess_images(images)
lang_x = tokenize_instruction(instruction)

# ëª¨ë¸ ì¶”ë¡ 
with torch.no_grad():
    outputs = model.forward(
        vision_x=vision_x,
        lang_x=lang_x,
        mode="inference"
    )

# ì•¡ì…˜ ë””ì½”ë”©
if discrete_actions:
    actions = action_tokenizer.decode_token_ids_to_actions(outputs.action_logits)
else:
    actions = outputs.action_predictions
```

### 3. ì•¡ì…˜ í›„ì²˜ë¦¬

```python
# VLA ì¶œë ¥ íŒŒì‹±
parser = VLAActionParser()
action = parser.parse_text_output(vla_output, original_prompt)

# ì•ˆì „ì„± ê²€ì¦
validator = ActionValidator(max_linear_speed=0.5, max_angular_speed=1.0)
safe_action = validator.validate_action(action)

# ë¡œë´‡ ì œì–´ ëª…ë ¹ ìƒì„±
if validator.is_safe_action(safe_action):
    robot_command = {
        'linear': {'x': safe_action.linear_x, 'y': safe_action.linear_y},
        'angular': {'z': safe_action.angular_z}
    }
```

## ğŸ“Š ì„±ëŠ¥ ë° íŠ¹ì§•

### 1. ì§€ì› ë°ì´í„°ì…‹
- **CALVIN**: ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½ì—ì„œì˜ ì¥ê¸° íƒœìŠ¤í¬
- **Open-X Embodiment**: ë‹¤ì–‘í•œ ë¡œë´‡ ë°ì´í„°ì…‹ í†µí•©
- **Custom Datasets**: ì‚¬ìš©ì ì •ì˜ ë°ì´í„°ì…‹ ì§€ì›

### 2. ì£¼ìš” íŠ¹ì§•
- **ë©€í‹°ëª¨ë‹¬ ìœµí•©**: ë¹„ì „, ì–¸ì–´, ì•¡ì…˜ì˜ í†µí•© ì²˜ë¦¬
- **ì‹œí€€ìŠ¤ ëª¨ë¸ë§**: ì‹œê°„ì  ì˜ì¡´ì„±ì„ ê³ ë ¤í•œ ì•¡ì…˜ ì˜ˆì¸¡
- **ìœ ì—°í•œ ì•¡ì…˜ ê³µê°„**: ì—°ì†/ì´ì‚° ì•¡ì…˜ ëª¨ë‘ ì§€ì›
- **í™•ì¥ ê°€ëŠ¥í•œ ì•„í‚¤í…ì²˜**: ìƒˆë¡œìš´ ë°±ë³¸ ëª¨ë¸ ì‰½ê²Œ ì¶”ê°€ ê°€ëŠ¥
- **ì•ˆì „ì„± ê²€ì¦**: ì•¡ì…˜ ìœ íš¨ì„± ë° ì•ˆì „ì„± ê²€ì‚¬

### 3. í›ˆë ¨ ìµœì í™”
- **í˜¼í•© ì •ë°€ë„**: BF16ì„ í†µí•œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
- **ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ…**: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”
- **ë¶„ì‚° í›ˆë ¨**: ë©€í‹° GPU ì§€ì›
- **ì ì§„ì  í•™ìŠµ**: ì‚¬ì „í›ˆë ¨ â†’ íŒŒì¸íŠœë‹ íŒŒì´í”„ë¼ì¸

## ğŸ”§ ì‹¤ì œ í™œìš© ì˜ˆì‹œ

### Jetson VLA í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ
```python
# jetson_vla_test.pyì—ì„œì˜ í™œìš©
def main():
    # ëª©í‘œ ì„¤ì •
    goal = Goal.FIND_OBJECT
    target_object = "cup"
    
    # VLA í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt = get_vlm_prompt(goal, target_object=target_object)
    
    # ëª¨ë¸ ì¶”ë¡ 
    vlm_output = model.generate(prompt, image)
    
    # ì•¡ì…˜ ì‹¤í–‰
    execute_action(goal, vlm_output, frame, target_object=target_object)
```

ì´ ë¶„ì„ì„ í†µí•´ RoboVLMsê°€ ì–´ë–»ê²Œ VLA ëª¨ë¸ì„ êµ¬í˜„í•˜ê³ , ì •ì±…ì„ í†µí•´ ì•¡ì…˜ì„ ê·œì •í•˜ë©°, ì‹¤ì œ ë¡œë´‡ ì œì–´ì— í™œìš©ë˜ëŠ”ì§€ ìƒì„¸íˆ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 