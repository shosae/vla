#!/usr/bin/env python3
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from std_msgs.msg import String
from geometry_msgs.msg import Twist
import torch
import cv2
import numpy as np
from PIL import Image as PilImage
from transformers import AutoProcessor, PaliGemmaForConditionalGeneration
from cv_bridge import CvBridge
import os
from pathlib import Path

class VLAInferenceNode(Node):
    def __init__(self):
        super().__init__('vla_node')
        self.get_logger().info("ğŸ¤– VLA ì¶”ë¡  ë…¸ë“œ ì´ˆê¸°í™” ì¤‘...")

        # ëª¨ë¸ ì„¤ì •
        self.declare_parameter('model_id', "google/paligemma-3b-mix-224")
        self.declare_parameter('model_cache_dir', ".vla_models_cache")
        self.declare_parameter('max_new_tokens', 128)
        self.declare_parameter('device_preference', "cuda")

        self.model_id = self.get_parameter('model_id').get_parameter_value().string_value
        self.model_cache_dir = self.get_parameter('model_cache_dir').get_parameter_value().string_value
        self.max_new_tokens = self.get_parameter('max_new_tokens').get_parameter_value().integer_value
        self.device_preference = self.get_parameter('device_preference').get_parameter_value().string_value

        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        if self.device_preference == "cuda" and torch.cuda.is_available():
            self.device = torch.device("cuda")
        else:
            self.device = torch.device("cpu")
        self.get_logger().info(f"ğŸ¯ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")

        # ëª¨ë¸ ë¡œë“œ
        self.model = None
        self.processor = None
        self.load_model()

        # ìƒíƒœ ë³€ìˆ˜
        self.bridge = CvBridge()
        self.current_image = None
        self.current_text = None
        self.image_width = 640
        self.image_height = 480

        # êµ¬ë…ì & ë°œí–‰ì (ê¸°ì¡´ ì¹´ë©”ë¼ ë…¸ë“œì™€ í˜¸í™˜)
        self.create_subscription(Image, '/camera/image_raw', self.image_callback, 10)
        self.create_subscription(String, '/stt/text', self.text_callback, 10)  # STT ê²°ê³¼ í† í”½
        
        # ì¶œë ¥ í† í”½
        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)
        self.status_pub = self.create_publisher(String, '/vla/status', 10)

        self.get_logger().info("âœ… VLA ì¶”ë¡  ë…¸ë“œ ì´ˆê¸°í™” ì™„ë£Œ")

    def load_model(self):
        """VLA ëª¨ë¸ ë¡œë“œ"""
        try:
            self.get_logger().info(f"ğŸ“¥ ëª¨ë¸ ë¡œë”© ì¤‘: {self.model_id}")
            
            model_save_path = Path(self.model_cache_dir) / self.model_id.split('/')[-1]
            model_save_path.mkdir(parents=True, exist_ok=True)

            # í”„ë¡œì„¸ì„œ ë¡œë“œ
            self.processor = AutoProcessor.from_pretrained(
                self.model_id, 
                cache_dir=model_save_path
            )

            # ëª¨ë¸ ë¡œë“œ
            model_kwargs = {
                "cache_dir": model_save_path,
                "low_cpu_mem_usage": True
            }
            
            if self.device.type == "cuda":
                model_kwargs["torch_dtype"] = torch.bfloat16
                model_kwargs["device_map"] = "auto"
            else:
                model_kwargs["torch_dtype"] = torch.float32

            self.model = PaliGemmaForConditionalGeneration.from_pretrained(
                self.model_id, 
                **model_kwargs
            )
            
            if self.device.type != "cuda":
                self.model.to(self.device)
            
            self.model.eval()
            self.get_logger().info("âœ… VLA ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            
        except Exception as e:
            self.get_logger().error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise

    def image_callback(self, msg):
        """ì¹´ë©”ë¼ ì´ë¯¸ì§€ ìˆ˜ì‹  (ê¸°ì¡´ ì¹´ë©”ë¼ ë…¸ë“œ í˜¸í™˜)"""
        try:
            self.current_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='rgb8')
            self.image_width = msg.width
            self.image_height = msg.height
            
            self.get_logger().debug(f"ğŸ“· ì´ë¯¸ì§€ ìˆ˜ì‹ : {self.image_width}x{self.image_height}")
            
            # ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ê°€ ëª¨ë‘ ìˆìœ¼ë©´ ì¶”ë¡  ì‹¤í–‰
            self.try_inference()
                
        except Exception as e:
            self.get_logger().error(f"âŒ ì´ë¯¸ì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

    def text_callback(self, msg):
        """STT í…ìŠ¤íŠ¸ ìˆ˜ì‹ """
        self.current_text = msg.data
        self.get_logger().info(f"ğŸ¯ STT í…ìŠ¤íŠ¸ ìˆ˜ì‹ : '{self.current_text}'")
        
        # ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ê°€ ëª¨ë‘ ìˆìœ¼ë©´ ì¶”ë¡  ì‹¤í–‰
        self.try_inference()

    def try_inference(self):
        """ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ê°€ ëª¨ë‘ ìˆìœ¼ë©´ VLA ì¶”ë¡  ì‹¤í–‰"""
        if self.current_image is None or self.current_text is None:
            return

        try:
            self.get_logger().info(f"ğŸ§  VLA ì¶”ë¡  ì‹¤í–‰: '{self.current_text}'")
            
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            pil_image = PilImage.fromarray(self.current_image)
            
            # í…ìŠ¤íŠ¸ ëª…ë ¹ì— ë”°ë¥¸ ì¶”ë¡ 
            linear_x, linear_y, angular_z = self.perform_vla_inference(pil_image, self.current_text)

            # Twist ë©”ì‹œì§€ ë°œí–‰
            self.publish_cmd_vel(linear_x, linear_y, angular_z)
            
            # ìƒíƒœ ë¦¬ì…‹ (í•œ ë²ˆ ì¶”ë¡  í›„ ì´ˆê¸°í™”)
            self.current_image = None
            self.current_text = None
            
        except Exception as e:
            self.get_logger().error(f"âŒ VLA ì¶”ë¡  ì˜¤ë¥˜: {e}")

    def perform_vla_inference(self, image, text_command):
        """VLA ëª¨ë¸ ì¶”ë¡ """
        try:
            # ê°„ë‹¨í•œ ëª…ë ¹ì–´ ë§¤í•‘ ë¨¼ì € í™•ì¸
            simple_commands = self.check_simple_commands(text_command)
            if simple_commands is not None:
                return simple_commands

            # VLA ëª¨ë¸ì„ í†µí•œ ë³µì¡í•œ ì¶”ë¡ 
            if "navigate to" in text_command.lower() or "go to" in text_command.lower():
                return self.perform_navigation_inference(image, text_command)
            elif "avoid" in text_command.lower() or "obstacle" in text_command.lower():
                return self.perform_obstacle_avoidance_inference(image, text_command)
            else:
                return self.perform_general_inference(image, text_command)
            
        except Exception as e:
            self.get_logger().error(f"âŒ VLA ì¶”ë¡  ì˜¤ë¥˜: {e}")
            return 0.0, 0.0, 0.0

    def check_simple_commands(self, text_command):
        """ê°„ë‹¨í•œ ëª…ë ¹ì–´ ì§ì ‘ ì²˜ë¦¬"""
        command_lower = text_command.lower()
        
        if "stop" in command_lower or "halt" in command_lower:
            self.get_logger().info("ğŸ›‘ ì •ì§€ ëª…ë ¹")
            return 0.0, 0.0, 0.0
        elif "move forward" in command_lower or "go forward" in command_lower:
            self.get_logger().info("â¡ï¸ ì „ì§„ ëª…ë ¹")
            return 0.3, 0.0, 0.0
        elif "move backward" in command_lower or "go backward" in command_lower:
            self.get_logger().info("â¬…ï¸ í›„ì§„ ëª…ë ¹")
            return -0.3, 0.0, 0.0
        elif "turn left" in command_lower:
            self.get_logger().info("â†ªï¸ ì¢ŒíšŒì „ ëª…ë ¹")
            return 0.0, 0.0, 0.5
        elif "turn right" in command_lower:
            self.get_logger().info("â†©ï¸ ìš°íšŒì „ ëª…ë ¹")
            return 0.0, 0.0, -0.5
        
        return None  # ë³µì¡í•œ ëª…ë ¹ì€ VLA ëª¨ë¸ ì‚¬ìš©

    def perform_navigation_inference(self, image, text_command):
        """ë‚´ë¹„ê²Œì´ì…˜ ì¶”ë¡ """
        try:
            # ëª©í‘œë¬¼ ì°¾ê¸°
            target = text_command.lower().replace("navigate to", "").replace("go to", "").strip()
            prompt = f"find {target} in the image and determine robot movement direction"
            
            inputs = self.processor(images=image, text=prompt, return_tensors="pt").to(self.device)
            
            with torch.no_grad():
                outputs = self.model.generate(**inputs, max_new_tokens=self.max_new_tokens)
                result = self.processor.decode(outputs[0], skip_special_tokens=True)
            
            self.get_logger().info(f"ğŸ¯ ë‚´ë¹„ê²Œì´ì…˜ ê²°ê³¼: {result}")
            return self.parse_action_to_twist(result)
            
        except Exception as e:
            self.get_logger().error(f"âŒ ë‚´ë¹„ê²Œì´ì…˜ ì¶”ë¡  ì˜¤ë¥˜: {e}")
            return 0.0, 0.0, 0.0

    def perform_obstacle_avoidance_inference(self, image, text_command):
        """ì¥ì• ë¬¼ íšŒí”¼ ì¶”ë¡ """
        try:
            prompt = "detect obstacles and suggest safe movement direction"
            inputs = self.processor(images=image, text=prompt, return_tensors="pt").to(self.device)
            
            with torch.no_grad():
                outputs = self.model.generate(**inputs, max_new_tokens=self.max_new_tokens)
                result = self.processor.decode(outputs[0], skip_special_tokens=True)
            
            # ì¥ì• ë¬¼ì´ ê°ì§€ë˜ë©´ ì•ˆì „í•œ ì†ë„ë¡œ ì´ë™
            if "obstacle" in result.lower() or "blocked" in result.lower():
                self.get_logger().info("ğŸ›‘ ì¥ì• ë¬¼ ê°ì§€ - ì •ì§€")
                return 0.0, 0.0, 0.0
            else:
                self.get_logger().info("âœ… ê²½ë¡œ ì•ˆì „ - ì²œì²œíˆ ì „ì§„")
                return 0.1, 0.0, 0.0
                
        except Exception as e:
            self.get_logger().error(f"âŒ ì¥ì• ë¬¼ íšŒí”¼ ì¶”ë¡  ì˜¤ë¥˜: {e}")
            return 0.0, 0.0, 0.0

    def perform_general_inference(self, image, text_command):
        """ì¼ë°˜ì ì¸ VLA ì¶”ë¡ """
        try:
            prompt = f"Robot action for command: {text_command}"
            inputs = self.processor(images=image, text=prompt, return_tensors="pt").to(self.device)
            
            with torch.no_grad():
                outputs = self.model.generate(**inputs, max_new_tokens=self.max_new_tokens)
                result = self.processor.decode(outputs[0], skip_special_tokens=True)
            
            self.get_logger().info(f"ğŸ¤– ì¼ë°˜ VLA ê²°ê³¼: {result}")
            return self.parse_action_to_twist(result)
            
        except Exception as e:
            self.get_logger().error(f"âŒ ì¼ë°˜ VLA ì¶”ë¡  ì˜¤ë¥˜: {e}")
            return 0.0, 0.0, 0.0

    def parse_action_to_twist(self, action_text):
        """VLA ê²°ê³¼ë¥¼ Twist ëª…ë ¹ìœ¼ë¡œ ë³€í™˜"""
        linear_x, linear_y, angular_z = 0.0, 0.0, 0.0
        
        action_lower = action_text.lower()
        
        if "forward" in action_lower or "ahead" in action_lower:
            linear_x = 0.2
        elif "backward" in action_lower or "back" in action_lower:
            linear_x = -0.2
        elif "left" in action_lower:
            angular_z = 0.5
        elif "right" in action_lower:
            angular_z = -0.5
        elif "stop" in action_lower or "halt" in action_lower:
            linear_x, linear_y, angular_z = 0.0, 0.0, 0.0
            
        return linear_x, linear_y, angular_z

    def publish_cmd_vel(self, linear_x, linear_y, angular_z):
        """cmd_vel ë©”ì‹œì§€ ë°œí–‰"""
        twist = Twist()
        twist.linear.x = linear_x
        twist.linear.y = linear_y
        twist.angular.z = angular_z
        
        self.cmd_vel_pub.publish(twist)
        self.get_logger().info(f"ğŸš€ cmd_vel ë°œí–‰: x={linear_x:.2f}, y={linear_y:.2f}, z={angular_z:.2f}")
        
        # ìƒíƒœ ë°œí–‰
        status_msg = String()
        status_msg.data = f"VLA inference completed: linear_x={linear_x:.2f}, linear_y={linear_y:.2f}, angular_z={angular_z:.2f}"
        self.status_pub.publish(status_msg)

def main(args=None):
    rclpy.init(args=args)
    
    try:
        node = VLAInferenceNode()
        rclpy.spin(node)
    except KeyboardInterrupt:
        pass
    except Exception as e:
        print(f"âŒ VLA ë…¸ë“œ ì˜¤ë¥˜: {e}")
    finally:
        rclpy.shutdown()

if __name__ == '__main__':
    main()
